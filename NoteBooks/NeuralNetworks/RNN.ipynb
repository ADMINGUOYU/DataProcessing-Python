{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RNN** - Recurrent Neural Network\n",
    "\n",
    "Recurrent Neural Network, is a type of deep learning model designed for processing sequential data, such as text or time-series, by using feedback loops that give the network a form of memory to retain and utilize information from previous inputs in a sequence. Key applications of RNNs include natural language processing tasks like translation and text generation, and they are often being replaced by more advanced transformer-based models for their greater efficiency.\n",
    "\n",
    "---\n",
    "\\* Based on **Stanford University CS231n: Deep Learning for Computer Vision** course material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzHZ5bMoFV_P",
    "outputId": "d64c6e2b-5d51-408d-abd9-c9686da405ac"
   },
   "outputs": [],
   "source": [
    "# This line of code helps you check current Python version\n",
    "# outputs: <version> (date & time) [C compiler]\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fully connected layer** (Linear Layer)\n",
    "\n",
    "A fully connected (FC) layer, also called a dense layer, is a core component of neural networks where every neuron in the layer connects to every neuron in the previous layer. These layers combine and transform the features learned by preceding layers, such as convolutional layers, to make complex global relationships and produce final predictions for tasks like classification or regression. Mathematically, an FC layer performs a linear transformation using weights and biases, followed by a nonlinear activation function, to generate its output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def affine_forward(x_input:numpy.ndarray, weight:numpy.ndarray, bias:numpy.ndarray) \\\n",
    "    -> typing.Tuple[numpy.ndarray, typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "\n",
    "    out = x_input.reshape(x_input.shape[0], -1).dot(weight) + bias # bias will be broadcasted to (1, M) -> compatable with (N, M)\n",
    "    cache = (x_input, weight, bias)\n",
    "\n",
    "    return (out, cache)\n",
    "\n",
    "def affine_backward(dout:numpy.ndarray, cache:typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine (fully connected) layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "\n",
    "    x, w, b = cache\n",
    "    dx:numpy.ndarray = dout.dot(w.T).reshape(x.shape)\n",
    "    dw:numpy.ndarray = x.reshape(x.shape[0], -1).T.dot(dout)\n",
    "    db:numpy.ndarray = numpy.sum(dout, axis = 0)\n",
    "    \"\"\"\n",
    "    x * W + '1' * b = out\n",
    "    db = id^(T) * dout = dout\n",
    "    \"\"\"\n",
    "    return (dx, dw, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vanilla RNN** - the basic one\n",
    "\n",
    "Vanilla RNN is easy to construct but with issues like exploding gradient or vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A single time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def rnn_step_forward(x_input:numpy.ndarray, prev_h:numpy.ndarray, Wx:numpy.ndarray, Wh:numpy.ndarray, b:numpy.ndarray) \\\n",
    "    -> typing.Tuple[numpy.ndarray, typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Run the forward pass for a single timestep of a vanilla RNN using a tanh activation function.\n",
    "    The input data has dimension D, the hidden state has dimension H,\n",
    "    and the minibatch is of size N.\n",
    "    Inputs:\n",
    "    - x: Input data for this timestep, of shape (N, D)\n",
    "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    next_h:numpy.ndarray = numpy.tanh(x_input.dot(Wx) + prev_h.dot(Wh) + b)\n",
    "    cache = (x_input, prev_h, Wx, Wh, next_h)\n",
    "\n",
    "    return (next_h, cache)\n",
    "\n",
    "def rnn_step_backward(dnext_h:numpy.ndarray, cache:typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of a vanilla RNN.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)\n",
    "    - cache: Cache object from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradients of input data, of shape (N, D)\n",
    "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
    "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradients of bias vector, of shape (H,)\n",
    "    \"\"\"\n",
    "\n",
    "    # unpack cache (type are automatically inferred)\n",
    "    x_input, prev_h, Wx, Wh, next_h = cache\n",
    "    # x_input (N, D)\n",
    "    # prev_h (N, H)\n",
    "    # Wx (D, H)\n",
    "    # Wh (H, H)\n",
    "    # bias (H,)\n",
    "\n",
    "    # calculate derivatives\n",
    "    # dnext_h (N, H)\n",
    "    dnext_h = dnext_h * (1 - (next_h ** 2))  # get inner derivative of tanh(X)\n",
    "    dx = numpy.dot(a = dnext_h, b = Wx.T)\n",
    "    dprev_h = numpy.dot(a = dnext_h, b = Wh.T)\n",
    "    dWx = numpy.dot(a = x_input.T, b = dnext_h)\n",
    "    dWh = numpy.dot(a = prev_h.T, b = dnext_h)\n",
    "    db = numpy.sum(a = dnext_h, axis = 0)\n",
    "\n",
    "    # return as tuple\n",
    "    return (dx, dprev_h, dWx, dWh, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More time-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def rnn_forward(x_input:numpy.ndarray, h0:numpy.ndarray, Wx:numpy.ndarray, Wh:numpy.ndarray, b:numpy.ndarray) \\\n",
    "    -> typing.Tuple[numpy.ndarray, typing.MutableSequence[typing.Tuple[numpy.ndarray, ...]]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data.\n",
    "\n",
    "    We assume an input sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running the RNN forward,\n",
    "    we return the hidden states for all timesteps. (Timestamp T)\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D)\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H)\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve shape info first\n",
    "    N, T, D = x_input.shape\n",
    "    _, H    = h0.shape\n",
    "\n",
    "    prev_h = h0 # just make a copy for readability\n",
    "    h = numpy.zeros(shape = (N, T, H)) # array to pack h (memories)\n",
    "    cache = []\n",
    "\n",
    "    # for every timestep\n",
    "    for i in range(T):\n",
    "        prev_h, cache_i = rnn_step_forward(x_input[:, i, :], prev_h, Wx, Wh, b)\n",
    "        h[:,i,:] = prev_h # pack and save H (will not save h0)\n",
    "        cache.append(cache_i)\n",
    "\n",
    "    # return\n",
    "    return (h, cache)\n",
    "\n",
    "def rnn_backward(dh:numpy.ndarray, cache:typing.MutableSequence[typing.Tuple[numpy.ndarray, ...]]) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
    "\n",
    "    NOTE: 'dh' contains the upstream gradients produced by the\n",
    "    individual loss functions at each timestep, *not* the gradients\n",
    "    being passed between timesteps (which you'll have to compute yourself\n",
    "    by calling rnn_step_backward in a loop).\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradient of biases, of shape (H,)\n",
    "    \"\"\"\n",
    "\n",
    "    # get shape info\n",
    "    N, T, H = dh.shape\n",
    "    x_input = cache[0][0]\n",
    "    _, D = x_input.shape\n",
    "\n",
    "    dx = numpy.zeros(shape = (N, T, D))\n",
    "    dh0 = numpy.zeros(shape = (N, H))\n",
    "    dWx = numpy.zeros(shape = (D, H))\n",
    "    dWh = numpy.zeros(shape = (H, H))\n",
    "    db = numpy.zeros(shape = (H,))\n",
    "\n",
    "    # loop\n",
    "    for i in reversed(range(T)):\n",
    "        ddx, dh0, ddWx, ddWh, ddb = rnn_step_backward(dh[:, i, :] + dh0, cache[i])\n",
    "        dx[:, i, :] += ddx\n",
    "        dWx += ddWx\n",
    "        dWh += ddWh\n",
    "        db += ddb\n",
    "\n",
    "    # return\n",
    "    return (dx, dh0, dWx, dWh, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Embedding**\n",
    "\n",
    "_A word embedding layer is a type of neural network layer that maps discrete word indices to dense, low-dimensional vectors of real numbers. These vectors, or \"embeddings,\" capture the semantic meaning and relationships between words, enabling models to process and understand text by learning from large text corpora. Key applications include NLP tasks like text classification, machine translation, and sentiment analysis, where similar words are positioned close to each other in the embedding space._\n",
    "\n",
    "**-> Basically, you convert your words into vectors. Or in other words, you put your words into a muti-dimensional space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DehRJyFtGDpb"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def word_embedding_forward(x:numpy.ndarray, W:numpy.ndarray) \\\n",
    "    -> typing.Tuple[numpy.ndarray, typing.Tuple[numpy.ndarray, numpy.ndarray]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass for word embeddings.\n",
    "\n",
    "    We operate on minibatches of size N where\n",
    "    each sequence has length T. We assume a vocabulary of V words, assigning each\n",
    "    word to a vector of dimension D.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Integer array of shape (N, T) giving indices of words. Each element idx\n",
    "    of x muxt be in the range 0 <= idx < V.\n",
    "    - W: Weight matrix of shape (V, D) giving word vectors for all words.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Array of shape (N, T, D) giving word vectors for all input words.\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "\n",
    "    out = W[x]\n",
    "    cache = (x, W)\n",
    "    return (out, cache)\n",
    "\n",
    "def word_embedding_backward(dout:numpy.ndarray, cache:typing.Tuple[numpy.ndarray, numpy.ndarray]) \\\n",
    "    -> numpy.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    Backward pass for word embeddings.\n",
    "\n",
    "    We cannot back-propagate into the words\n",
    "    since they are integers, so we only return gradient for the word embedding\n",
    "    matrix.\n",
    "    HINT: Look up the function numpy.add.at\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream gradients of shape (N, T, D)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    - dW: Gradient of word embedding matrix, of shape (V, D)\n",
    "    \"\"\"\n",
    "\n",
    "    # unpack cache\n",
    "    x, W = cache # shape of x: (N, T)\n",
    "    # create zeroed return value\n",
    "    dW = numpy.zeros_like(a = W) # shape (V, D)\n",
    "    # put dout to correct places of word embedding matrix\n",
    "    \"\"\"\n",
    "    numpy.add.at is a NumPy function that performs an \"unbuffered\" in-place addition\n",
    "    operation on elements of an array a at specified indices.\n",
    "    It is part of the numpy.ufunc.at family of functions,\n",
    "    which provide a way to apply a universal function (ufunc) at specific locations within an array.\n",
    "    \"\"\"\n",
    "    numpy.add.at(dW, x, dout) # dW[x] is in the shape of (N, T, D)\n",
    "    # return\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sigmoid** - normalization function\n",
    "\n",
    "-> You map $\\mathbb{R}$ to $(0, 1)$\n",
    "\n",
    "-> Sigmoid function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqh0aH8q0iRh"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def sigmoid(x:numpy.ndarray) -> numpy.ndarray:\n",
    "    \"\"\"A numerically stable version of the logistic sigmoid function.\"\"\"\n",
    "    # avoid MAX out\n",
    "    # x >= 0 : 1 / (1 + e ^ (-x))\n",
    "    # x < 0 : e ^ (x) / (1 + e ^ (x))\n",
    "    pos_mask = x >= 0\n",
    "    neg_mask = x < 0\n",
    "    z = numpy.zeros_like(a = x)\n",
    "    z[pos_mask] = numpy.exp(-x[pos_mask])\n",
    "    z[neg_mask] = numpy.exp(x[neg_mask])\n",
    "    top = numpy.ones_like(x)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    return top / (1 + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM RNN** - Long Short-Term Memory RNN\n",
    "\n",
    "LSTM, or Long Short-Term Memory, is a type of recurrent neural network (RNN) designed to process sequential data by remembering important information over long periods and forgetting irrelevant details, thus overcoming the vanishing gradient problem common in standard RNNs. It uses a sophisticated internal structure with memory cells and gates (forget, input, and output) to selectively store and retrieve information, making it effective for tasks like speech recognition, language translation, and time-series prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A single time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def lstm_step_forward(x_input:numpy.ndarray, prev_h:numpy.ndarray, prev_c:numpy.ndarray, Wx:numpy.ndarray, Wh:numpy.ndarray, b:numpy.ndarray) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray, typing.Tuple[numpy.ndarray, ...]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass for a single timestep of an LSTM.\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "    Note that a sigmoid() function has already been provided for you in this file.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, D)\n",
    "    - prev_h: Previous hidden state, of shape (N, H)\n",
    "    - prev_c: previous cell state, of shape (N, H)\n",
    "    - Wx: Input-to-hidden weights, of shape (D, 4H)\n",
    "    - Wh: Hidden-to-hidden weights, of shape (H, 4H)\n",
    "    - b: Biases, of shape (4H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - next_c: Next cell state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve shape of H\n",
    "    H = prev_h.shape[1]\n",
    "\n",
    "    # forward pass\n",
    "    a:numpy.ndarray = x_input.dot(Wx) + prev_h.dot(Wh) + b\n",
    "\n",
    "    # calculate gates and attributes\n",
    "    i = sigmoid(a[ : , : H])                    # input intake ratio\n",
    "    f = sigmoid(a[ : , H : 2 * H])              # forget ratio\n",
    "    o = sigmoid(a[ : , 2 * H : 3 * H])          # output ratio of next_c\n",
    "    g = numpy.tanh(a[ : , 3 * H : 4 * H])       # intermediate result (comprehending input)\n",
    "\n",
    "    # calculate next long short term state\n",
    "    next_c = f * prev_c + i * g\n",
    "    next_h = o * numpy.tanh(next_c)\n",
    "\n",
    "    # prepare cache\n",
    "    cache = (x_input, prev_h, prev_c, next_c, Wx, Wh, i, f, g, o)\n",
    "\n",
    "    # return\n",
    "    return (next_h, next_c, cache)\n",
    "\n",
    "def lstm_step_backward(dnext_h:numpy.ndarray, dnext_c:numpy.ndarray, cache:typing.Tuple[numpy.ndarray, ...]) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of an LSTM.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradients of next hidden state, of shape (N, H)\n",
    "    - dnext_c: Gradients of next cell state, of shape (N, H)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "     Returns a tuple of:\n",
    "    - dx: Gradient of input data, of shape (N, D)\n",
    "    - dprev_h: Gradient of previous hidden state, of shape (N, H)\n",
    "    - dprev_c: Gradient of previous cell state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)\n",
    "    - db: Gradient of biases, of shape (4H,)\n",
    "    \"\"\"\n",
    "\n",
    "    # unpack cache\n",
    "    x_input, prev_h, prev_c, next_c, Wx, Wh, i, f, g, o = cache\n",
    "\n",
    "    # get shape\n",
    "    N = prev_h.shape[0]\n",
    "    H = prev_h.shape[1]\n",
    "\n",
    "    # prepare da\n",
    "    da = numpy.zeros((N, 4 * H))\n",
    "\n",
    "    # previously:\n",
    "    # C = Ct-1 * f + i * g (N, H)\n",
    "    # H = o * tanh(C) (N, H)\n",
    "    # (1 - tanh ** 2) == (sech ** 2) which is the derivative of tanh\n",
    "    dnext_c += dnext_h * o * (1 - numpy.tanh(next_c) ** 2)  # get real gradient (sum) -> as c's gradient is accumulated (initially 0)\n",
    "                                                            # since C doesn't involve in the calculation flow of x\n",
    "    dprev_c = dnext_c * f                                   # dnext_c / dprev_c = f while dLoss / dc = dnext_c\n",
    "    df = dnext_c * prev_c\n",
    "    di = dnext_c * g\n",
    "    dg = dnext_c * i\n",
    "    do = dnext_h * numpy.tanh(next_c)\n",
    "\n",
    "    # put separate parts into da\n",
    "    # derivative of sigmoid = sigmoided * (1 - sigmoided)\n",
    "    da[ : , : H] = di * i * ( 1 - i )\n",
    "    da[ : , H : 2 * H] = df * f * ( 1 - f )\n",
    "    da[ : , 2 * H : 3 * H] = do * o * ( 1 - o )\n",
    "    da[ : , 3 * H : 4 * H] = dg * ( 1 - g ** 2)             # this one comes from tanh -> derivative = sech ^ 2\n",
    "\n",
    "    # calculate the rest\n",
    "    db = numpy.sum(da, axis = 0)                            # since everywhere same bias\n",
    "    dx = numpy.dot(da, Wx.T)\n",
    "    dWx = numpy.dot(x_input.T, da)\n",
    "    dprev_h = numpy.dot(da, Wh.T)\n",
    "    dWh = numpy.dot(prev_h.T, da)\n",
    "\n",
    "    # return\n",
    "    return (dx, dprev_h, dprev_c, dWx, dWh, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More time-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MR5XyPkW9uFT"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def lstm_forward(x_input:numpy.ndarray, h0:numpy.ndarray, Wx:numpy.ndarray, Wh:numpy.ndarray, b:numpy.ndarray) \\\n",
    "    -> typing.Tuple[numpy.ndarray, typing.List[typing.Tuple[numpy.ndarray, ... ]]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass for an LSTM over an entire sequence of data.\n",
    "\n",
    "    We assume an input sequence composed of T vectors, each of dimension D. The LSTM uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running the LSTM forward,\n",
    "    we return the hidden states for all timesteps.\n",
    "    Note that the initial cell state is passed as input, but the initial cell state is set to zero.\n",
    "    Also note that the cell state is not returned; it is an internal variable to the LSTM and is not\n",
    "    accessed from outside.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, T, D)\n",
    "    - h0: Initial hidden state of shape (N, H)\n",
    "    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)\n",
    "    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)\n",
    "    - b: Biases of shape (4H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    # get shape\n",
    "    N, T, D = x_input.shape\n",
    "    N, H = h0.shape\n",
    "\n",
    "    h = numpy.zeros((N, T, H))\n",
    "    cache = []\n",
    "\n",
    "    # initially, h = h0, c = 0\n",
    "    h_i = h0\n",
    "    c_i = numpy.zeros((N, H))\n",
    "\n",
    "    # for every time step\n",
    "    for i in range(T):\n",
    "        h_i, c_i, cache_i = lstm_step_forward(x_input[:, i, :], h_i, c_i, Wx, Wh, b)\n",
    "        h[:, i, :] = h_i\n",
    "        cache.append(cache_i)\n",
    "\n",
    "    return h, cache\n",
    "\n",
    "def lstm_backward(dh:numpy.ndarray, cache:typing.List[typing.Tuple[numpy.ndarray, ... ]]) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Backward pass for an LSTM over an entire sequence of data.\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of hidden states, of shape (N, T, H)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input data of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)\n",
    "    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)\n",
    "    - db: Gradient of biases, of shape (4H,)\n",
    "    \"\"\"\n",
    "\n",
    "    # get shape\n",
    "    N, T, H = dh.shape\n",
    "    D = cache[0][0].shape[1]\n",
    "\n",
    "    dx = numpy.zeros(shape = (N, T, D))\n",
    "    dWx = numpy.zeros(shape = (D, 4 * H))\n",
    "    dWh = numpy.zeros(shape = (H, 4 * H))\n",
    "    db = numpy.zeros(shape = 4 * H)\n",
    "    dh_i = numpy.zeros(shape = (N, H))\n",
    "    dc_i = numpy.zeros(shape = (N, H))\n",
    "\n",
    "    # in reversed order\n",
    "    for i in range(T - 1, -1, -1):\n",
    "        dh_i += dh[:, i, :]\n",
    "        dx_i, dh_i, dc_i, dWx_i, dWh_i, db_i = lstm_step_backward(dh_i, dc_i, cache[i])\n",
    "        dx[:, i, :] = dx_i\n",
    "        dWx += dWx_i\n",
    "        dWh += dWh_i\n",
    "        db += db_i\n",
    "\n",
    "    dh0 = dh_i\n",
    "\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fully connected layer** - with time-steps in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC-nocWm6cxd"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def temporal_affine_forward(x_input:numpy.ndarray, w:numpy.ndarray, b:numpy.ndarray) \\\n",
    "    -> typing.Tuple[numpy.ndarray, typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass for a temporal affine layer.\n",
    "\n",
    "    The input is a set of D-dimensional vectors arranged into a minibatch of N\n",
    "    timeseries, each of length T. We use an affine function to transform each\n",
    "    of those vectors into a new vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, T, D)\n",
    "    - w: Weights of shape (D, M)\n",
    "    - b: Biases of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, M)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve shape info\n",
    "    N, T, D = x_input.shape\n",
    "    M = b.shape[0]\n",
    "\n",
    "    # performs transformation\n",
    "    out:numpy.ndarray = x_input.reshape((N * T, D)).dot(b = w).reshape((N, T, M)) + b\n",
    "    cache:typing.Tuple = (x_input, w, b, out)\n",
    "\n",
    "    # return\n",
    "    return (out, cache)\n",
    "\n",
    "def temporal_affine_backward(dout:numpy.ndarray, cache:typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Backward pass for temporal affine layer.\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream gradients of shape (N, T, M)\n",
    "    - cache: Values from forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input, of shape (N, T, D)\n",
    "    - dw: Gradient of weights, of shape (D, M)\n",
    "    - db: Gradient of biases, of shape (M,)\n",
    "    \"\"\"\n",
    "\n",
    "    # unpack data\n",
    "    x_input, w, b, out = cache\n",
    "\n",
    "    # obtain shape info\n",
    "    N, T, D = x_input.shape\n",
    "    M = b.shape[0]\n",
    "\n",
    "    # calculate derivatives\n",
    "    dx:numpy.ndarray = dout.reshape((N * T, M)).dot(b = w.T).reshape((N, T, D))\n",
    "    dw:numpy.ndarray = (x_input.reshape((N * T, D)).T).dot(dout.reshape((N * T, M)))\n",
    "    db:numpy.ndarray = dout.sum(axis = (0, 1))\n",
    "\n",
    "    # return\n",
    "    return (dx, dw, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Softmax Loss function** - with time-steps in mind\n",
    "\n",
    "The Softmax Loss function, more formally known as Categorical Cross-Entropy Loss, measures the difference between a model's predicted probability distribution and the true probability distribution for a multi-class classification problem. It is the standard loss function paired with the Softmax activation function, as the softmax function outputs probabilities that the loss function then compares to the actual labels. The goal is to minimize this loss, guiding the model to assign high probabilities to the correct classes and low probabilities to incorrect classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvQGa-hw6cxd"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "def temporal_softmax_loss(x:numpy.ndarray, y:numpy.ndarray, mask:numpy.ndarray, verbose:bool = False) \\\n",
    "    -> typing.Tuple[numpy.ndarray, numpy.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    A temporal version of softmax loss for use in RNNs.\n",
    "\n",
    "    We assume that we are making predictions over a vocabulary of size V for each timestep of a\n",
    "    timeseries of length T, over a minibatch of size N. The input x gives scores for all vocabulary\n",
    "    elements at all timesteps, and y gives the indices of the ground-truth element at each timestep.\n",
    "    We use a cross-entropy loss at each timestep, summing the loss over all timesteps and averaging\n",
    "    across the minibatch.\n",
    "    As an additional complication, we may want to ignore the model output at some timesteps, since\n",
    "    sequences of different length may have been combined into a minibatch and padded with NULL\n",
    "    tokens. The optional mask argument tells us which elements should contribute to the loss.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input scores, of shape (N, T, V)\n",
    "    - y: Ground-truth indices, of shape (N, T) where each element is in the range\n",
    "    0 <= y[i, t] < V\n",
    "    - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n",
    "    the scores at x[i, t] should contribute to the loss.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving loss\n",
    "    - dx: Gradient of loss with respect to scores x.\n",
    "    \"\"\"\n",
    "\n",
    "    # get shape info and flat N * T\n",
    "    N, T, V = x.shape\n",
    "    x_flat = x.reshape((N * T, V))\n",
    "    y_flat = y.reshape((N * T))\n",
    "    mask_flat = mask.reshape((N * T))\n",
    "\n",
    "    # calculate loss\n",
    "    probs = numpy.exp(x_flat - numpy.max(x_flat, axis = 1, keepdims = True))    # for every [V], utilizes broadcasting\n",
    "    probs /= numpy.sum(probs, axis = 1, keepdims = True)\n",
    "    loss:numpy.ndarray = -numpy.sum(mask_flat * numpy.log(probs[numpy.arange(N * T), y_flat])) / N  # only sums unmasked region\n",
    "\n",
    "    # calculate derivative\n",
    "    dx_flat = probs.copy()\n",
    "    dx_flat[numpy.arange(N * T), y_flat] -= 1\n",
    "    dx_flat /= N                            # dx_flat shape (N * T, V)\n",
    "    dx_flat *= mask_flat[ : , None ]        # use None to skip a dimension (N * T, 1) -> making broadcasting compatable\n",
    "\n",
    "    if verbose:\n",
    "        print(\"dx_flat: \", dx_flat.shape)\n",
    "\n",
    "    # reshape back\n",
    "    dx:numpy.ndarray = dx_flat.reshape((N, T, V))\n",
    "\n",
    "    # return\n",
    "    return (loss, dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpfQEF726cxm"
   },
   "source": [
    "### **Classifier Class** - A class that has it all\n",
    "\n",
    "-> Example from course 231n, a img2txt RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import typing\n",
    "\n",
    "class CaptioningRNN:\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\t\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            word_to_idx:typing.Dict[str, int],\n",
    "            input_dim:int = 512,\n",
    "            wordvec_dim:int = 128,\n",
    "            hidden_dim:int = 128,\n",
    "            cell_type:str = \"rnn\",\n",
    "            dtype:numpy.dtype = numpy.float32\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Construct a new CaptioningRNN instance.\t\n",
    "        Inputs:\n",
    "         - word_to_idx: A dictionary giving the vocabulary. It contains V entries,\n",
    "         - and maps each string to a unique integer in the range [0, V).\n",
    "         - input_dim: Dimension D of input image feature vectors.\n",
    "         - wordvec_dim: Dimension W of word vectors.\n",
    "         - hidden_dim: Dimension H for the hidden state of the RNN.\n",
    "         - cell_type: What type of RNN to use; either 'rnn' or 'lstm'.\n",
    "         - dtype: numpy datatype to use; use float32 for training and float64 for\n",
    "         - numeric gradient checking.\n",
    "        \"\"\"\t\n",
    "        # check RNN type\n",
    "        if cell_type not in {\"rnn\", \"lstm\"}:\n",
    "            raise ValueError('Invalid cell_type \"%s\"' % cell_type)\n",
    "\n",
    "        # assign member variables\n",
    "        self.cell_type:str                          = cell_type\n",
    "        self.dtype:numpy.dtype                      = dtype\n",
    "        self.word_to_idx:typing.Dict[str, int]      = word_to_idx\n",
    "        self.idx_to_word:typing.Dict[int, str]      = {i: w for w, i in word_to_idx.items()}\n",
    "        self.params:typing.Dict[str, numpy.ndarray] = {}\t\n",
    "        # protected member variables\n",
    "        self._null:int                              = word_to_idx[\"<NULL>\"]\n",
    "        self._start:int                             = word_to_idx.get(\"<START>\", None)\n",
    "        self._end:int                               = word_to_idx.get(\"<END>\", None)\t\n",
    "        # attribute\n",
    "        vocab_size:int                              = len(word_to_idx)\t\n",
    "        # Initialize word vectors\n",
    "        # numpy.random.randn(): The distribution has a mean of 0 and a variance of 1.\n",
    "        self.params[\"W_embed\"] = numpy.random.randn(vocab_size, wordvec_dim)\n",
    "        self.params[\"W_embed\"] /= 100\t\n",
    "        # Initialize CNN -> hidden state projection parameters\n",
    "        # using fully connected layers\n",
    "        self.params[\"W_proj\"] = numpy.random.randn(input_dim, hidden_dim)\n",
    "        self.params[\"W_proj\"] /= numpy.sqrt(input_dim)\n",
    "        self.params[\"b_proj\"] = numpy.zeros(hidden_dim)\t\n",
    "        # Initialize parameters for the RNN\n",
    "        dim_mul:int = {\"lstm\": 4, \"rnn\": 1}[cell_type]\n",
    "        self.params[\"Wx\"] = numpy.random.randn(wordvec_dim, dim_mul * hidden_dim)\n",
    "        self.params[\"Wx\"] /= numpy.sqrt(wordvec_dim)\n",
    "        self.params[\"Wh\"] = numpy.random.randn(hidden_dim, dim_mul * hidden_dim)\n",
    "        self.params[\"Wh\"] /= numpy.sqrt(hidden_dim)\n",
    "        self.params[\"b\"] = numpy.zeros(dim_mul * hidden_dim)\t\n",
    "        # Initialize output to vocab weights\n",
    "        self.params[\"W_vocab\"] = numpy.random.randn(hidden_dim, vocab_size)\n",
    "        self.params[\"W_vocab\"] /= numpy.sqrt(hidden_dim)\n",
    "        self.params[\"b_vocab\"] = numpy.zeros(vocab_size)\n",
    "\n",
    "        # Cast parameters to correct dtype\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(self.dtype)\n",
    "\n",
    "    def loss(self, features:numpy.ndarray, captions:numpy.ndarray):\n",
    "        \"\"\"\n",
    "        Compute training-time loss for the RNN. We input image features and\n",
    "        ground-truth captions for those images, and use an RNN (or LSTM) to compute\n",
    "        loss and gradients on all parameters.\n",
    "\n",
    "        Inputs:\n",
    "        - features: Input image features, of shape (N, D)\n",
    "        - captions: Ground-truth captions; an integer array of shape (N, T + 1) where\n",
    "                    each element is in the range 0 <= y[i, t] < V\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar loss\n",
    "        - grads: Dictionary of gradients parallel to self.params\n",
    "        \"\"\"\t\n",
    "        # Cut captions into two pieces: captions_in has everything but the last word\n",
    "        # and will be input to the RNN; captions_out has everything but the first\n",
    "        # word and this is what we will expect the RNN to generate. These are offset\n",
    "        # by one relative to each other because the RNN should produce word (t+1)\n",
    "        # after receiving word t. The first element of captions_in will be the START\n",
    "        # token, and the first element of captions_out will be the first word.\n",
    "        captions_in = captions[ : , : -1 ]\n",
    "        captions_out = captions[ : , 1 : ]\n",
    "        # You'll need this\n",
    "        mask = (captions_out != self._null)\n",
    "        # Weight and bias for the affine transform from image features to initial\n",
    "        # hidden state\n",
    "        W_proj, b_proj = self.params[\"W_proj\"], self.params[\"b_proj\"]\n",
    "        # Word embedding matrix\n",
    "        W_embed = self.params[\"W_embed\"]\n",
    "        # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n",
    "        Wx, Wh, b = self.params[\"Wx\"], self.params[\"Wh\"], self.params[\"b\"]\n",
    "        # Weight and bias for the hidden-to-vocab transformation.\n",
    "        W_vocab, b_vocab = self.params[\"W_vocab\"], self.params[\"b_vocab\"]\n",
    "        loss, grads = 0.0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward and backward passes for the CaptioningRNN.   #\n",
    "        # In the forward pass you will need to do the following:                   #\n",
    "        # (1) Use an affine transformation to compute the initial hidden state     #\n",
    "        #     from the image features. This should produce an array of shape (N, H)#\n",
    "        # (2) Use a word embedding layer to transform the words in captions_in     #\n",
    "        #     from indices to vectors, giving an array of shape (N, T, W).         #\n",
    "        # (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #\n",
    "        #     process the sequence of input word vectors and produce hidden state  #\n",
    "        #     vectors for all timesteps, producing an array of shape (N, T, H).    #\n",
    "        # (4) Use a (temporal) affine transformation to compute scores over the    #\n",
    "        #     vocabulary at every timestep using the hidden states, giving an      #\n",
    "        #     array of shape (N, T, V).                                            #\n",
    "        # (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #\n",
    "        #     the points where the output word is <NULL> using the mask above.     #\n",
    "        #                                                                          #\n",
    "        #                                                                          #\n",
    "        # Do not worry about regularizing the weights or their gradients!          #\n",
    "        #                                                                          #\n",
    "        # In the backward pass you will need to compute the gradient of the loss   #\n",
    "        # with respect to all model parameters. Use the loss and grads variables   #\n",
    "        # defined above to store loss and gradients; grads[k] should give the      #\n",
    "        # gradients for self.params[k].                                            #\n",
    "        #                                                                          #\n",
    "        # Note also that you are allowed to make use of functions from layers.py   #\n",
    "        # in your implementation, if needed.                                       #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # forward pass\n",
    "        initial_state, cache_affine = affine_forward(features, W_proj, b_proj)\n",
    "        word_embed, cache_embed = word_embedding_forward(captions_in, W_embed)\n",
    "        if self.cell_type == \"rnn\":\n",
    "            h, cache_forward = rnn_forward(word_embed, initial_state, Wx, Wh, b)\n",
    "        elif self.cell_type == \"lstm\":\n",
    "            h, cache_forward = lstm_forward(word_embed, initial_state, Wx, Wh, b)\n",
    "        out_vocab, cache_vocab = temporal_affine_forward(h, W_vocab, b_vocab)\n",
    "        loss, dout = temporal_softmax_loss(out_vocab, captions_out, mask)\n",
    "        # backward pass\n",
    "        dh, dW_vocab, db_vocab = temporal_affine_backward(dout, cache_vocab)\n",
    "        if self.cell_type==\"rnn\":\n",
    "            dword_embed, dinitial_state, dWx, dWh, db = rnn_backward(dh, cache_forward)\n",
    "        elif self.cell_type==\"lstm\":\n",
    "            dword_embed, dinitial_state, dWx, dWh, db = lstm_backward(dh, cache_forward)\n",
    "        dW_embed = word_embedding_backward(dword_embed, cache_embed)\n",
    "        dfeatures, dW_proj, db_proj = affine_backward(dinitial_state, cache_affine)\n",
    "        grads['W_proj'] = dW_proj\n",
    "        grads['b_proj'] = db_proj\n",
    "        grads['W_embed'] = dW_embed\n",
    "        grads['Wx'] = dWx\n",
    "        grads['Wh'] = dWh\n",
    "        grads['b'] = db\n",
    "        grads['W_vocab'] = dW_vocab\n",
    "        grads['b_vocab'] = db_vocab\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "        return loss, grads\t\n",
    "        \n",
    "    def sample(self, features:numpy.ndarray, max_length:int = 30):\n",
    "        \"\"\"\n",
    "        Run a test-time forward pass for the model, sampling captions for input\n",
    "        feature vectors.\n",
    "        At each timestep, we embed the current word, pass it and the previous hidd\n",
    "        state to the RNN to get the next hidden state, use the hidden state to get\n",
    "        scores for all vocab words, and choose the word with the highest score as\n",
    "        the next word. The initial hidden state is computed by applying an affine\n",
    "        transform to the input image features, and the initial word is the <START>\n",
    "        token.\n",
    "        For LSTMs you will also have to keep track of the cell state; in that case\n",
    "        the initial cell state should be zero.\n",
    "\n",
    "        Inputs:\n",
    "        - features: Array of input image features of shape (N, D).\n",
    "        - max_length: Maximum length T of generated captions.\n",
    "\n",
    "        Returns:\n",
    "        - captions: Array of shape (N, max_length) giving sampled captions,\n",
    "                    where each element is an integer in the range [0, V). The first element\n",
    "                    of captions should be the first sampled word, not the <START> token.\n",
    "        \"\"\"\n",
    "\n",
    "        N:int = features.shape[0]\n",
    "        captions = self._null * numpy.ones((N, max_length), dtype = numpy.int32)\n",
    "        # Unpack parameters\n",
    "        W_proj, b_proj = self.params[\"W_proj\"], self.params[\"b_proj\"]\n",
    "        W_embed = self.params[\"W_embed\"]\n",
    "        Wx, Wh, b = self.params[\"Wx\"], self.params[\"Wh\"], self.params[\"b\"]\n",
    "        W_vocab, b_vocab = self.params[\"W_vocab\"], self.params[\"b_vocab\"]\n",
    "        ###########################################################################\n",
    "        # TODO: Implement test-time sampling for the model. You will need to      #\n",
    "        # initialize the hidden state of the RNN by applying the learned affine   #\n",
    "        # transform to the input image features. The first word that you feed to  #\n",
    "        # the RNN should be the <START> token; its value is stored in the         #\n",
    "        # variable self._start. At each timestep you will need to do to:          #\n",
    "        # (1) Embed the previous word using the learned word embeddings           #\n",
    "        # (2) Make an RNN step using the previous hidden state and the embedded   #\n",
    "        #     current word to get the next hidden state.                          #\n",
    "        # (3) Apply the learned affine transformation to the next hidden state to #\n",
    "        #     get scores for all words in the vocabulary                          #\n",
    "        # (4) Select the word with the highest score as the next word, writing it #\n",
    "        #     (the word index) to the appropriate slot in the captions variable   #\n",
    "        #                                                                         #\n",
    "        # For simplicity, you do not need to stop generating after an <END> token #\n",
    "        # is sampled, but you can if you want to.                                 #\n",
    "        #                                                                         #\n",
    "        # HINT: You will not be able to use the rnn_forward or lstm_forward       #\n",
    "        # functions; you'll need to call rnn_step_forward or lstm_step_forward in #\n",
    "        # a loop.                                                                 #\n",
    "        #                                                                         #\n",
    "        # NOTE: we are still working over minibatches in this function. Also if   #\n",
    "        # you are using an LSTM, initialize the first cell state to zeros.        #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        state, _ = affine_forward(features, W_proj, b_proj)\n",
    "        word = self._start * numpy.ones(N, dtype = numpy.int8)\n",
    "        if self.cell_type == \"lstm\":\n",
    "            c = numpy.zeros_like(state)\n",
    "        for i in range(max_length):\n",
    "            word, _ = word_embedding_forward(word, W_embed)\n",
    "            if self.cell_type==\"rnn\":\n",
    "                state, _ = rnn_step_forward(word, state, Wx, Wh, b)\n",
    "            elif self.cell_type == \"lstm\":\n",
    "                state, c, _ = lstm_step_forward(word, state, c, Wx, Wh, b)\n",
    "            word_scores, _ = affine_forward(state, W_vocab, b_vocab)\n",
    "            word = word_scores.argmax(axis = 1)\n",
    "            captions[ : , i ] = word\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "        return captions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
